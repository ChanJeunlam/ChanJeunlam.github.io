---
layout: post
title: "机器学习入门（一）：线性代数"
date: 2022-12-14 
description: "介绍机器学习入门需要的线性代数知识，如矩阵转置、范数等等"
tag: 机器学习 线性代数
---   

# 00 写在前面
如何将ipynb文件转为markdown文件？
```shell
jupyter nbconvert --to markdown note.ipynb
```

# chapter 01


```python
import numpy as np
# 向量 
v = np.array([1,2,3])
# 矩阵
m = np.array([[1,2,3],[4,5,6],[7,8,9]])
# 张量
t = np.array([
    [[1,2,3],[4,5,6],[7,8,9]],
    [[11,12,13],[14,15,16],[17,18,19]],
    [[21,22,23],[24,25,26],[27,28,29]],
])
print("向量："+str(v))
print("矩阵："+str(m))
print("张量："+str(t))
```

    向量：[1 2 3]
    矩阵：[[1 2 3]
     [4 5 6]
     [7 8 9]]
    张量：[[[ 1  2  3]
      [ 4  5  6]
      [ 7  8  9]]
    
     [[11 12 13]
      [14 15 16]
      [17 18 19]]
    
     [[21 22 23]
      [24 25 26]
      [27 28 29]]]
    

## 1.2 矩阵转置



```python
a = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]])
a_t = a.transpose()
print(a)
# print one line to separate the two outputs
print('-'*30)
print(a_t)
```

    [[ 1  2  3]
     [ 4  5  6]
     [ 7  8  9]
     [10 11 12]]
    ------------------------------
    [[ 1  4  7 10]
     [ 2  5  8 11]
     [ 3  6  9 12]]
    

## 1.3 矩阵加法
有时候允许矩阵和向量相加，得到一个矩阵，本质上是构造了一个将b按行复制的一个新矩阵，这种操作叫做广播（broadcasting）。


## 1.4 矩阵乘法
矩阵乘法的结果是一个矩阵，其第i行第j列的元素是矩阵A的第i行与矩阵B的第j列的内积。A的形状为m×n，B的形状为n×p，那么A×B的形状为m×p。矩阵乘法的计算可以用下面的公式表示：
    $$c[i,j] = a[i,:] * b[:,j]$$


```python
m1 = np.array([[1.0,3.0],[1.0,0.0]])
m2 = np.array([[1.0,2.0],[3.0,5.0]])
print(m1)
print(m2)
# 矩阵乘法
m3 = np.dot(m1,m2)
print(m3)
# 矩阵按元素相乘
m4 = np.multiply(m1,m2)
# 矩阵按元素相乘
m5 = m1*m2
print(m4)
print(m5)
```

    [[1. 3.]
     [1. 0.]]
    [[1. 2.]
     [3. 5.]]
    [[10. 17.]
     [ 1.  2.]]
    [[1. 6.]
     [3. 0.]]
    [[1. 6.]
     [3. 0.]]
    

## 1.5 单位矩阵
单位矩阵是一个方阵，对角线上的元素为1，其余元素为0。单位矩阵的作用是不改变矩阵的值，它乘以任何矩阵都等于原矩阵。


```python
np.identity(3)
```




    array([[1., 0., 0.],
           [0., 1., 0.],
           [0., 0., 1.]])



## 1.6 矩阵的逆
矩阵A的逆（inverse）是一个矩阵B，使得A×B=I，其中I是单位矩阵。矩阵A的逆记为$A^{−1}$。如果A是一个方阵，那么A的逆也是一个方阵。如果A的逆存在，那么A的逆也叫做A的伪逆（pseudo-inverse）。


```python
A = [[1,2],[4,5]]
A_inv = np.linalg.inv(A) 
print(A_inv)
```

    [[-1.66666667  0.66666667]
     [ 1.33333333 -0.33333333]]
    

## 1.7范数 norm
向量$L^P$范数定义为：
    $$||x||_p = (\sum_{i=1}^n |x_i|^p)^{1/p},p\geq 1,p\in\mathbb{R}$$

L1范数：向量中各个元素绝对值之和
    $$||x||_1 = \sum_{i=1}^n |x_i|$$
L0范数：向量中非零元素的个数
    $$||x||_0 = \sum_{i=1}^n \mathbb{1}(x_i\neq 0)$$
L2范数：向量中各个元素平方和的平方根,也叫欧式范数,是向量x到原点的欧几里得距离，有时候也用L2范数的来衡量向量：$x^Tx$.
    $$||x||_2 = \sqrt{\sum_{i=1}^n x_i^2}$$
L∞范数：向量中各个元素绝对值的最大值
    $$||x||_\infty = \max_{i=1,\cdots,n}|x_i|$$

机器学习中常用的范数有L1范数和L2范数，L1范数用于稀疏性的优化，L2范数用于凸优化。
